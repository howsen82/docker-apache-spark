version: '3.9'

# https://medium.com/@mehmood9501/using-apache-spark-docker-containers-to-run-pyspark-programs-using-spark-submit-afd6da480e0f
services:
  spark-master:
    image: bitnami/spark:latest
    container_name: spark
    ports:
      - 8080:8080
      - 7077:7077
    command: bin/spark-class org.apache.spark.deploy.master.Master

  spark-worker-1:
    image: bitnami/spark:latest
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_MASTER_URL=spark://spark-master:7077
  
  spark-worker-2:
    image: bitnami/spark:latest
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_MASTER_URL=spark://spark-master:7077
